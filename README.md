# **👐 AI-Powered Sign Language Recognition**  
**Enhancing accessibility with AI and embedded systems**  

### **🚀 Project Overview**  
This project leverages **computer vision, deep learning, and embedded hardware** to recognize **American Sign Language (ASL) hand gestures** in real time. The goal is to bridge communication gaps by translating ASL gestures into **text and speech**, making conversations more accessible for the **Deaf and Hard-of-Hearing (DHH) community**.  

The initial phase focuses on **AI-powered sign recognition using OpenCV and TensorFlow**, while the future roadmap includes **hardware integration with Arduino and flex sensors** to build a **wearable assistive device**.

---

## **🔹 Features**  
✅ **Computer Vision-Based Sign Detection** – AI recognizes ASL hand gestures from a webcam.  
✅ **Deep Learning for Gesture Recognition** – Trained with a pre-existing ASL dataset for accuracy.  
✅ **Real-Time Translation** – Converts sign language gestures into **on-screen text** or **speech output**.  
✅ **Assistive Technology Focus** – Designed to **increase accessibility** for non-verbal communication.  
✅ **Future Hardware Integration** – Next phase includes **Arduino + flex sensors** for a wearable glove.  

---

## **🛠️ Technologies Used**  
### **AI & Software**  
- **Python** – Primary programming language  
- **TensorFlow / Keras** – Model training and prediction  
- **OpenCV** – Real-time hand tracking  
- **MediaPipe** – Hand landmark detection  
- **Numpy & Pandas** – Data preprocessing  

### **Embedded Systems & Hardware (Future Roadmap)**  
- **Arduino Uno / ESP32** – Microcontroller for gesture sensing  
- **Flex Sensors** – Detect finger movement in gloves  
- **Bluetooth Module (HC-05)** – Wireless communication between gloves and the AI model  
- **OLED Display / Speaker** – Output device for displaying or speaking detected gestures  

---

## **🛠️ Project Structure (To Be Updated)**  
```
/sign-language-ai
│── models/                  # Trained AI models  
│── datasets/                # ASL dataset used for training  
│── src/                     # Python scripts for detection & classification  
│── hardware/                # Arduino code for future integration  
│── notebooks/               # Jupyter notebooks for experiments  
│── README.md                # Project documentation  
│── requirements.txt         # Dependencies  
```

---

## **🚧 Project Status**  
📌 **Current Phase:**  
✅ Training **AI model for sign recognition** (using OpenCV + TensorFlow).  
✅ Developing **real-time webcam detection** for ASL gestures.  

📌 **Next Steps:**  
🔹 Improve model accuracy with **data augmentation and fine-tuning**.  
🔹 Integrate **text-to-speech (TTS)** for voice output.  
🔹 Prototype **Arduino-based glove with flex sensors** for hardware integration.  

---

## **📸 Demo Screenshots (To Be Added)**  
🔹 Hand detection example  
🔹 ASL gesture classification output  
🔹 Future hardware prototype preview  

---

## **💡 Why This Project?**  
🌍 **Inclusion & Accessibility:** Sign language is a vital means of communication for the Deaf community, but **many people don’t understand ASL**. This project aims to **bridge that gap using AI and assistive tech**.  

🔬 **AI & Embedded Systems Integration:** Combining **machine learning with hardware** opens new doors for **real-world impact in accessibility technologies**.  

💻 **Open-Source Contribution:** The goal is to make this **an open-source project** so that others can contribute and improve the model.  

---

## **📌 How to Run the Project (Coming Soon)**  
1. **Clone the repository**  
   ```bash
   git clone https://github.com/ashleyprado/sign-language-ai
   cd sign-language-ai
   ```
2. **Install dependencies**  
   ```bash
   pip install -r requirements.txt
   ```
3. **Run real-time ASL detection script**  
   ```bash
   python src/detect_signs.py
   ```

---

## **🤝 Contributing**  
Interested in contributing to this project? Feel free to fork the repo, submit a pull request, or reach out to collaborate!  

---

## **📬 Contact & Updates**  
👩‍💻 **Project Lead:** Ashley Prado  
📩 **Email:** ashleypradoceleste@gmail.com  
🔗 **GitHub Repo:** [GitHub Link] (Replace this with the actual repo link)  
📌 **WeCode Harvard 2025 Presentation**  

---

## **🌟 Future Vision**  
This project is more than just a software model—it’s a **step toward making sign language translation more accessible through AI and wearable technology.** By combining **deep learning and embedded systems**, this solution has the potential to evolve into **a real-time sign language translator for everyday use.**  
\
